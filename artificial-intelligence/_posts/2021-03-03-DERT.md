---
레ㄴRelayout: post
title: 【Transformer+OD】DETR-End-to-End Object Detection with Transformers w/ advice
---

- **논문** : [End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872.pdf)  
  필기 완료된 파일은 `OneDrive\21.1학기\논문읽기 `에 있다.
- **분류** :  Transformer + Object Detection
- 저자 : Nicolas Carion, Francisco Massa, Gabriel Synnaeve
- **읽는 배경** : Visoin Transformers 를 사용한 Object Detection
- **느낀점** :
  - **Conclution -> Method -> Introduction -> Abstract** 순서로 논문을 공부하자! 
  - **실제 공부 순서**    
    1. Conclusion, Abstract
    2. Introduction :  Fig 1 과 Fig 1을 설명하는 논문의 부분. 1문단만 읽음
    3. Introduction : 거의 마지막 부분
    4. Related work : 큰 제목이 의미하는 것이 무엇인지만, 알아봤다. 
    5. The DETR model
  - 논문이 정말 깔끔하다. 이전의 ViT (16x16 vision transfomer) 논문을 보다가, 이 논문을 보니까 문장도 정말 잘 써놨고 이해도 정말 잘되게 써놨다. 일부러 어렵게 써놓지도 않아서 너무 좋다. 이렇게 아주아주 유명하고 유용한 논문도 이렇게 쉽게 써 놓을 수 있고, 쉽게 이해가 될 수 있는데.... 내가 논문을 읽으면서 **"어렵고, 이해가 안되고, 논문 읽기 싫다" 라는 생각이 들때면, 이 DERT 논문을 떠올리자. 논문을 개같이 써놔서 이해가 안되고, 어려운거지, 코드를 보고 실제를 확인하면 정말 별거 없다.** 내가 부족해서 어려운거 라고 생각하며 우울해하지말고, **"논문을 개같이 써서 어려운거다! 이 논문만 그런거다! 또 다른 논문은 쉽게 이해되고 재미있을 수 있다!"** 라고 생각하자. **왜 우울해 해. 모르면 배우면 되지. 생각하지 말고 그냥 하자.**
- 목차 :
  1. Paper
  2. Code
     - https://github.com/facebookresearch/detr



---

---

# End-to-End Object Detection with Transformers

![image-20210303203616333](C:\Users\sb020\AppData\Roaming\Typora\typora-user-images\image-20210303203616333.png)

---

# 1. Conclusion, Abstract, Introduction

- **이 논문의 핵심 2가지** 
  1. Transformers(encoder-decoder architectures)를 기반으로 한, object detection systems. 
  2. Direct <u>set prediction</u> (NMS 따위 안쓰고, Class, BB 한방에 예측해버림) problem 관점으로, <u>bipartite(Predicted BB and GT) matching을 수행하는, set-based global loss</u> 를 정의해 사용했다.
  3. Fixed small set of learned <u>object queries</u> 를 사용한다. : DERT가 '객체와 Global Image context' 와의 관계를 추론하여, Parallel하게 final predictions을 하게 만든다.
- **특장점**
  1. <u>Powerful</u> :  optimized Faster R-CNN 와 유사한 성능 결과를 가진다.
  2. <u>straightforward, simple</u> : 
     - detection pipeline을 간소화했다. 코드가 단순하다. 
     - Task에 필요한 prior knowledge(사전적 지식 = 인간의 지식 = Heuristics Network)을 정확히 Encoding해야하는, 많은 hand-designed components (= NMS, anchor generation)  를 제거했다. 
     - any customized layers (heuristic한 접근법들) 을 사용하지 않았다. 
     - 한방에 모든 객체를 예측한다. (Direct set prediction)
  3. <u>flexible</u> : 원래의 DERT 모델에서, simple segmentation head 를 추가함으로써, 쉽게 panoptic segmentation 구현했다.
  4. <u>large objects</u> : self-attention을 활용해서, global information을 이용해서, 큰 객체 아주 예측을 잘한다. 
  5. 추가 특징 
     - extra-long training schedule 이 필요하다. 
     - auxiliary/parallel decoding losses 으로 부터 오는 이익이 있다. 
- **해결해야 할 도전과제들(challenge)**
  - <u>small objects</u>



---

# 2. Related work

- **2.1 Set Prediction** (한방에 예측 세트를 찾아내는 것 = directly predict sets)
- **2.2 Transformers and Parallel Decoding** (여기서 Parallel Decoding이 아직 뭔지 모르겠다. 하지만 Method를 보면 이해될 것 같다. 이름만 대단한 것 같지, 모델을 보면 Parallel 이 뭘 의미하는지 알 수 있을 거다.)
- **2.3 Object detection**
  - Set-based loss
  - Recurrent detectors.





---

# 3. The DETR model

## 3.1 Object detection set prediction loss

- DERT에서는 Single pass through the decoder를 통해서 N개의 고정된 크기 Predictions을 추출한다. 여기서 N은 평균 이미지 내부의 객체 수 보다 훨씬 크다. 
- 학습 중에 GT에 따른 예측값이 얼마나 잘 뽑혔는지 점수를 매겨야한다. 
- **(1) optimal bipartite matching**
  - ![image-20210304153546840](C:\Users\sb020\AppData\Roaming\Typora\typora-user-images\image-20210304153546840.png)
  - 위 이미지에서 Hungarian algorithm 이 뭔지는 정확히 모르겠다. 노동자와 일이 있을 때, 노동자에게 일을 할당하는 방법을 고르는 알고리즘이란다.(O(n!)의 경우의 수 시간을, O(n^3)으로 바꾸는 방법) 정말 필요하면, [43] 논문을 보면 될 것 같다.
  - 맨 아래의 수식처럼, P_i(c_i) 즉 just probabilities 값을 사용한게 아니라 log-probabilities  값을 사용했다. 경험적으로 더 좋은 성능일 보여서 log-probabilities를 최종 Loss 수식으로 정했다. 
- **(2) object-specific (bounding box) losses = Bounding box loss**
  - L1을 사용하면, relative scaling of the loss 문제점이 발생한다. : 만약 각각 2개의 GT BB와 Predicted BB가 있다고 하나. 하나는 박스가 Large하고 하나는 Small 하다. 만약 L1 loss를 사용하면, 2개의 BB pair에 따른 IOU가 비슷하다고 하더라도, Large Box에 대한 Loss가 더 크기 마련이다. 
  - 그래서 linear combination of the L1 loss and the generalized IoU loss 를 사용한다. 
  - <img src="C:\Users\sb020\AppData\Roaming\Typora\typora-user-images\image-20210304154317185.png" alt="image-20210304154317185" style="zoom:90%;" />



---

## 3.2 DETR architecture

- surprisingly simple
  - transformer architecture implementation with just a few hundred lines (코드 몇줄에 네트워크 완성 가능)
  -  Inference code for DETR can be implemented in less than 50 lines (코드 몇줄에 Inference 가능)
- **Transformer encoder.**
  - transformer architecture는 permutation-invariant 치환불변 하기 때문에(순서에 대한 정보가 없이 Input이 들어가기 때문에), fixed positional encoding을 추가해 준다.
- **Transformer decoder.**
  - 그림의 Encoder 부분에는 x N이라고 적혀있다. ~~이게 누적적으로 이뤄지는게 아니라, 이미지의 인풋이 계속들어가며 독립적으로 N개의 dxWH feature를 만들어 내고, 각각의 dxWH가 Decoder에 들어가는 건가? 코드를 보면서 확인해야 할 듯하다.~~ 코드를 보니, 누적되어 결과가 나온다. 예를 들어서 Feature map input이 dxHW이면, Encoder를 계속 반복해서 최종 Encoding 값으로  (차원은 모르겠고) N개가 나온다. 그리고 이게 Decoder로 들어가서 (NLP는 Sequencail하게 결과값이 Decoder의 맨아래 outpus로 들어가지만 여기서는 그런거 없고) 한방에 N개의 Prediction 결과가 나온다.  
  - Object query : N 명의 사람들 각각이, 자기가 하고 싶은 질문은 한다. 고 상상하라. 예를 들어서 첫번째 query가 묻는다. '헤이 이미지! 아래 왼쪽에 있는 객체는 뭐야?' 이런한 질문을 N번 받고, 그 결과가 N번 출력된다. 그게 Prediction set 이다. 또한 N명의 사람들은 Multi-Head Self-Attention을 통과하면서 서로서로 Communication 한다. 그래서 나오는 최종 질문이 Decoder의 Multi-Head Attention으로 들어간다.     
    <img src="C:\Users\sb020\AppData\Roaming\Typora\typora-user-images\image-20210304181917273.png" alt="image-20210304181917273" style="zoom:80%;" />
  - 따라서 아래의 그림을 보면, 하나의 query에서 하나의 결과가 나오는 것을 알 수 있다. 1사람이 물어본 것에 대해서 1가지 답변을 해주니까! 
- **Prediction feed-forward networks (FFNs).**
  - 3-layer perceptron with ReLU activation function
  - ∅ is used to represent that no object (다른 OD에서 the “background” class라고 예측하는 것과 비슷하다.)
- **Auxiliary decoding losses.**
  - use auxiliary losses [1] in decoder during training.

![T6](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/2021-1/Transformer_note6.png?raw=true)



---

# 4. Experiments

<img src="C:\Users\sb020\AppData\Roaming\Typora\typora-user-images\image-20210304211644160.png" alt="image-20210304211644160" style="zoom:80%;" />

- DERT = ResNet50 을 backbone으로 가짐. R101 = ResNet101. DC5 = 마지막 Feature map의 Resolution을 높혀서 Small object에서도 강한 성능을 보이게 만들기 위해서, dilation  을 마지막 Layer에 추가하고( (dilated C5 stage)), 초반 conv에 stride를 적용하지 않는다.

---

## 4.2 Ablations

- ResNet-50-based DETR model 에 대해서 최종적으로 6 encoder, 6 decoder layers and width 256를 골랐다. 
- **Number of encoder layers**
  1. 위의 Table2 이미지 보기. Encoder의 사용 갯수를 다르게 해봄으로써, "global image level self-attention"이 얼마나 중요한지 확인할 수 있었다. 
  2. <u>encoder 과정에서  global scene reasoning (전체 장면 추론)이 이뤄지고, objects들끼리 disentangling (서로 풀림) 이 이뤄진다고 추론했다.</u> 
  3. 아래 이미지는, 마지막 encoder layer에서 출력되는 attention maps을 시각화한 것이다. (아래 이미지를 어떤 방식으로 시각홰 했는지 모르겠다.)    
     ![image-20210304224435901](C:\Users\sb020\AppData\Roaming\Typora\typora-user-images\image-20210304224435901.png)
- **Number of decoder layers**
  1. 6개의 decoder layer 각각을 한 후에 auxiliary losses 를 수행했다. (위에 설명 및 논문 참조)
  2. 위의 Table4 이미지 보기. Decoder를 통해서 Encoding 결과와 Object query간의 소통(communication)이 이뤄지고, cross-correlations를 서로서로 파악해가며, 성능이 좋아진다.
  3. NMS를 사용하면, 초반에는 중복 예측 중 좋은 예측을 골라주므로 성능이 좋아지지만, Depth가 증가하면 NMS에 의한 성능 증가가 작아지는 것을 확인했다. 마지막에는 NMS가 성능을 더 낮추는 일이 발생해서 NMS를 쓰지 않는 것으로 결정했다.     
  4. Decoder에서도 어떻게 attention이 이뤄지는지 시각화를 해 놓았다. 객체의 머리 팔다리 사지(extremities) 에 집중하는 것을 확인할 수 있다.    
     ![image-20210304225511150](C:\Users\sb020\AppData\Roaming\Typora\typora-user-images\image-20210304225511150.png)
  5. <u>추론하기를, Encoder에서 global attention을 이용해, Instance의 분리가 이뤄지고, decoder에서는 class and object boundaries를 검출하기 위해서, 객체의 팔, 다리, 머리 (extremities)에 집중한다.</u> 
- **Importance of FFN**
  1. ㅇㄴㄹ





























