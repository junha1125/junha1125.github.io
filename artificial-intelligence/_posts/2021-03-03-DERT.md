---
레ㄴRelayout: post
title: 【Transformer+OD】DETR-End-to-End Object Detection with Transformers w/ advice
---

- **논문** : [End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872.pdf)  
  필기 완료된 파일은 `OneDrive\21.1학기\논문읽기 `에 있다.
- **분류** :  Transformer + Object Detection
- 저자 : Nicolas Carion, Francisco Massa, Gabriel Synnaeve
- **읽는 배경** : Visoin Transformers 를 사용한 Object Detection
- **느낀점** :
  - **Conclution -> Method -> Introduction -> Abstract** 순서로 논문을 공부하자! 
  - **실제 공부 순서**    
    1. Conclusion, Abstract
    2. Introduction :  Fig 1 과 Fig 1을 설명하는 논문의 부분. 1문단만 읽음
    3. Introduction : 거의 마지막 부분
    4. Related work : 큰 제목이 의미하는 것이 무엇인지만, 알아봤다. 
    5. (3.2) DERT architecture : 3.1은 Loss에 관한 이야기이다. set prediction loss가 여기서 굉장히 중요한듯 하나, 일단 내가 Architecture가 더 궁금하니, 여기서 부터 읽었다.
  - 논문이 정말 깔끔하다. 이전의 ViT (16x16 vision transfomer) 논문을 보다가, 이 논문을 보니까 문장도 정말 잘 써놨고 이해도 정말 잘되게 써놨다. 일부러 어렵게 써놓지도 않아서 너무 좋다. 이렇게 아주아주 유명하고 유용한 논문도 이렇게 쉽게 써 놓을 수 있고, 쉽게 이해가 될 수 있는데.... 내가 논문을 읽으면서 **"어렵고, 이해가 안되고, 논문 읽기 싫다" 라는 생각이 들때면, 이 DERT 논문을 떠올리자. 논문을 개같이 써놔서 이해가 안되고, 어려운거지, 코드를 보고 실제를 확인하면 정말 별거 없다.** 내가 부족해서 어려운거 라고 생각하며 우울해하지말고, **"논문을 개같이 써서 어려운거다! 이 논문만 그런거다! 또 다른 논문은 쉽게 이해되고 재미있을 수 있다!"** 라고 생각하자. **왜 우울해 해. 모르면 배우면 되지. 생각하지 말고 그냥 하자.**
- 목차 :
  1. Paper
  2. Code
     - https://github.com/facebookresearch/detrreasons 



# End-to-End Object Detection with Transformers

![image-20210303203616333](C:\Users\sb020\AppData\Roaming\Typora\typora-user-images\image-20210303203616333.png)

---

# 1. Conclusion, Abstract, Introduction

- **이 논문의 핵심 2가지** 
  1. Transformers(encoder-decoder architectures)를 기반으로 한, object detection systems. 
  2. Direct <u>set prediction</u> problem 관점으로, <u>bipartite(Predicted BB and GT) matching을 수행하는, set-based global loss</u> 를 정의해 사용했다.
  3. Fixed small set of learned <u>object queries</u> : DERT가 '객체와 Global Image context' 와의 관계를 추론하여, Parallel하게 final predictions을 하게 만든다.
- **특장점**
  1. <u>Powerful</u> :  optimized Faster R-CNN 와 유사한 성능 결과를 가진다.
  2. <u>straightforward, simple</u> : 
     - detection pipeline을 간소화했다. 코드가 단순하다. 
     - Task에 필요한 prior knowledge(사전적 지식 = 인간의 지식 = Heuristics Network)을 정확히 Encoding해야하는, 많은 hand-designed components (= NMS, anchor generation)  를 제거했다. 
     - any customized layers 를 사용하지 않았다. 
     - 한방에 모든 객체를 예측한다. 
  3. <u>flexible</u> : 원래의 DERT 모델에서, simple segmentation head 를 추가함으로써, 쉽게 panoptic segmentation 구현했다.
  4. <u>large objects</u> : self-attention을 활용해서, global information을 이용해서, 큰 객체 아주 예측을 잘한다. 
  5. 추가 특징 
     - extra-long training schedule 이 필요하다. 
     - auxiliary+parallel decoding losses 으로 부터 오는 이익이 있다. 
- **해결해야 할 도전과제들(challenge)**
  - <u>small objects</u>



---

# 2. Related work

- **2.1 Set Prediction** (한방에 예측 세트를 찾아내는 것 = directly predict sets)
- **2.2 Transformers and Parallel Decoding** (여기서 Parallel Decoding이 아직 뭔지 모르겠다. 하지만 Method를 보면 이해될 것 같다. 이름만 대단한 것 같지, 모델을 보면 Parallel 이 뭘 의미하는지 알 수 있을 거다.)
- **2.3 Object detection**
  - Set-based loss
  - Recurrent detectors.





---

# 3. The DETR model

## 3.2 DETR architecture

- surprisingly simple
- transformer architecture implementation with just a few hundred lines (코드 몇줄에 네트워크 완성 가능)
-  Inference code for DETR can be implemented in less than 50 lines (코드 몇줄에 Inference 가능)
- **Transformer encoder.**
  1. transformer architecture는 permutation-invariant 치환불변 하기 때문에, fixed positional encoding을 추가해 주었다.(?)



## 3.1 Object detection set prediction loss

































