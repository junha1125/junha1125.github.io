---
layout: post
title: 【기계학습】기계학습 핵심정리 chapter1-소개, chapter2-수학
description: >
    기계학습 책을 공부하고, 핵심만 기록해 놓는다. 구체적인 그림 및 설명은 최소화 하고, 꼭 필요한 설명은 1줄 이내로 적는다. 
    마음가짐 : '책 100번 볼거다. 또 보면 된다. 핵심만 정리해라'
---

기계학습 책 핵심 정리 chapter 1,2,3,4

# Chapter 01 - 소개

- 1.1절 - 기계학습이란

- 1.2절 - feature(input) demention의 가시화 생각해보기

- 1.3절 - 특징 공간 변환 공식 찾기 = 신경망 학습 알고리즘

- 1.4절 - 기계 학습 알고리즘 - Θ = argmin_Θ loss(Θ)

- **1.5절 - 모델선택**
    1. 1d -> 1d로 이해하는 underfitting과 overfitting
    2. bias와 variance   
        - Train Data set의 변화에 따른 예측 모델들 간의 관계
    3. validation set과 cross-validation
        - validation 알고리즘 흐름 참조(50p 알고리즘1-4)

- **1.6절 - 규제**
    1. data argumentation
    2. weight decay 
        - Loss = L1(x,y) + wights

- **1.7절 - 기계 학습 유형**
    1. 지도, 비지도, 준비도, 강화
    2. oneline/offline  
        deterministic/stochastic  
        discriminative/generative  

- 1.8절 - 기계학습의 과거, 현재, 미래
    - 다층퍼셉트론 -> SVM -> Deep learning
    - 쫄지말자. 복잡도만 다를 뿐 기본 원리는 같다. 

# Chapter 02 - 수학
## 2.1절 - 선형대수
- Norm, 단위백터, Cosine similarity(코사인 유사도=단위백터화 후 내적)
- 퍼셉트론과 행렬곱 Output(cx1) = Weight(cxd) x Input(dx1)
- 행렬에서 행백터들(혹은 열백터들)이 선형 결합, 선형독립과 선형종속
- mxn행렬에 대해 rank가 d(\<= n)이면 행백터들에 대해, d차원 백터공간을 표현할 수 있다.
- mxn행렬에 대해 최대 rank는 n(=행백터의 차원) 이다. 
- 역행렬이 없는 행렬 = singular matrix
- 역행렬을 가진다 = 최대 계수 = 모든 열 선형 독립 = 고윳값은 모두 0이 아니다.
- 모든 행렬은 (1)양의 정부호, (2)양의 준 정부호, (3)음의 정부호, (4)음의 준 정부호, (5)부정부호 행렬 5가지로 분류 할 수 있다. 분류 방법은 88p
- 이 부호를 가지는 행렬은 각각 고윳값이 (1)모두 양수, (2)0이거나 양수, (3)음수, (4)0이거나 음수, (5)양수와 음수, 이다. 
- 고윳값과 고유백터, 그리고 고유윳값 분해(약수분해, 인수분해 처럼! 정사각행렬일때), 특잇값 분해(정사각행렬이 아니어도) 
- 모든 고유 백터는 orthogonal한다.

## 2.2절 - 확률과 통계
- 확률분포, 곱과 합의 법칙, 조건부 확률, 두 사건의 독립
- 베이즈 정리 
    - 하얀 공이 나왔다는 사실만 알고 어느 병에서 나왔는지 모르는데, 어느 병인지 추정해라. 
    - 사후 확률(사건이 벌어진 후 확률) = P( Label | 데이터-Input Feature ) = 알기 어려움
- 우도(likelihood) = P( 알고있음,데이터 | 추정해야함,모델/Label ) = 그나마 쉬움.
- 최대 우도법(Maximum likelihood) = argmaxP( 데이터x | 모델 )  
    데이터 x가 주어졌을 때, x를 발생시켰을 가능성을 최대로 하는 매개변수(모델)
- 최대 로그 우도 추정
- 평균과 분산 공식, d차원 feature에 대한, d개의 평균, dxd 차원 공분산 행렬(105p)
- 가우시안 분포(=정규 분포), 베르누이 분포(1번 실험) -> 이항분포(m번 실험)
- 정보이른 : 메세지가 가진 정보량을 수량화하여 '수'로 나타낼 수 있을까?  
    발생활 확률이 적은 사건일 수록, 정보량이 크다!
- 엔트로피 : 한(P) 확률분포의 무질서도, 불확실성 측정 = 클 수록 예측하기 어렵다.(윷놀이<<주사위)
    - Cross 엔트로피 : 두(P,Q) 확률분포의 무질서도. -> 아래와 같이 분리가능 (교환법칙 성립)
    - P와 Q의 Cross 엔트로피 = P의 엔트로피(얼마나 예측하기 힘든지) + P와 Q사이의 KL diversence
    - KL diversence : 두 확률분포가 얼마나 다른지? 다를 수록 큰 수 (교환법칙 성립 안함)
    - cross 엔트로피 장점은 더 큰 오류에 더 낮은 벌점 주는 경우(=Mean Squared제곱 Error) 발생 안함(5장 추가 참조)





## 2.3절 - 최적화 이론


